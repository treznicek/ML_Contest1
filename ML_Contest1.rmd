---
title: "Competition 1"
author: "Tim Reznicek"
date: "2024-3-15"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r, echo=FALSE}
# Libraries
if (!requireNamespace("dplyr", quietly = TRUE)) install.packages("dplyr")
if (!require(glmnet)) install.packages("glmnet")
library(glmnet)
library(igraph)
library(plotly)
library(dplyr)
library(lubridate)
library(ggplot2)
library(zoo)
library(tidyr)
library(broom)
library(randomForest)
library(stringr)

```

```{r}
# orders dataframe
orders <- read.csv("contest1data/orders.csv")
orders$date <- as.Date(orders$date)      # Making sure this data is date format
str(orders)
summary(orders)
```

```{r}
# items dataframe
items <- read.csv("contest1data/items.csv")
str(items)

```

```{r}
# categories dataframe
categories <- read.csv("contest1data/categories.csv", stringsAsFactors = FALSE)
str(categories)

```

```{r}
# test dataframe
test <- read.csv("contest1data/test.csv")
str(test)
summary(test)
```


```{r}
# how many manufacturerIDs exist, what is the most common?
summary(items$manufacturerID)
mode(items$manufacturerID)
```

```{r}
# Excluding empty values, summary of f columns
sapply(items[, c("f1", "f2", "f3", "f4", "f5")], function(x) summary(x[x != -1]))
```

```{r}
# how many missing values exist in each column?
missing_counts <- sapply(items, function(x) sum(is.na(x) | x == -1 | x == ""))
print(missing_counts)
```

```{r}
# missingness relation between features
with(items, table(category == "", f5 == -1))
with(items, table(category == "", f4 == -1))
with(items, table(category == "", f3 == -1))
with(items, table(f3 == -1, f5 == -1))
with(items, table(f4 == -1, f5 == -1))
with(items, table(f4 == -1, f3 == -1))
```

```{r}
# category column
cleaned_categories <- gsub("]", "", as.character(items$category[!is.na(items$category) & items$category != ""]))
# Split the cleaned categories into individual categories
categories <- strsplit(cleaned_categories, ",")
# Now, you can proceed to find unique categories and count them as before
unique_categories <- unique(unlist(categories))
category_counts <- table(unlist(categories))

# Print the counts of each category
#print(category_counts)

print("could reorder this to show the most represented at the top or pie chart or something.")
```

```{r}
# g <- graph_from_data_frame(d = categories, directed = TRUE)
# pdf("category_hierarchy.pdf", width = 50, height = 10)
# plot(g, layout = layout_as_tree(g), edge.arrow.size = 0.5, vertex.label = V(g)$name, vertex.size = 10, vertex.label.cex = 0.4)
# dev.off()
```

```{r}
# # Assuming 'g' is your igraph object
# coords <- layout_as_tree(g)  # Get coordinates for tree layout
# edge_x <- c(); edge_y <- c()
# for(e in E(g)){
#   # Extract the vertex ids for the ends of each edge
#   ends_ids <- ends(g, e, names = FALSE)
#   
#   # Use these ids to index into coords directly
#   edge_x <- c(edge_x, coords[ends_ids[1], 1], coords[ends_ids[2], 1], NA)
#   edge_y <- c(edge_y, coords[ends_ids[1], 2], coords[ends_ids[2], 2], NA)
# }
# 
# node_x <- coords[,1]
# node_y <- coords[,2]
# text_labels <- V(g)$name
# 
# # Create edges
# p <- plot_ly(type='scatter', mode='lines', x=~edge_x, y=~edge_y, line=list(color='black')) %>%
#   add_trace(x=~node_x, y=~node_y, mode='markers+text', text=~text_labels, textposition='bottom center', hoverinfo='text')
# 
# # Customize layout
# p <- layout(p, title='Category Hierarchy')
# 
# # Display the plot
# p
```


```{r}
items_filtered <- items
items_filtered[items_filtered == -1] <- NA  # Replace -1 with NA for filtering

# Select only the columns f1 to f5
features <- items_filtered[,c("f1", "f2", "f3", "f4", "f5")]

#scatterplots of all combinations of f1 to f5
pairs(features, panel = panel.smooth, main = "Scatterplots of Features f1 to f5")
```

Taking into account both the missingness relationships, how many values are missing in each feature, and the relationships between features, I have devised the following method to clean the features.  
f1: make them 0  
f2: no missing  
f3 and f4 have high overlap in missingness
f3: median (400 something)
f4: median (0)
f5: median as well, not sure if I can do any good predictions on this.  

What about categories? To start out I will consider missing categories to be empty. Eventually I would like to find features related to categories and potentially fill them in that way.  

Join orders and items on itemID.  

```{r}
# Join the datasets on 'itemID'
joined_data <- inner_join(orders, items, by = "itemID")
```

### Cleaning f1-f5
```{r}
joined_data$f1[joined_data$f1 == -1] <- 0

# Calculate median for f3, excluding -1
median_f3 <- median(joined_data$f3[joined_data$f3 != -1], na.rm = TRUE)
# Replace -1 with median in f3
joined_data$f3[joined_data$f3 == -1] <- median_f3

# f4
median_f4 <- median(joined_data$f4[joined_data$f4 != -1], na.rm = TRUE)
joined_data$f4[joined_data$f4 == -1] <- median_f4

# f5
median_f5 <- median(joined_data$f5[joined_data$f5 != -1], na.rm = TRUE)
joined_data$f5[joined_data$f5 == -1] <- median_f5
```


### Feature Extraction from Date:
```{r}
# create a new column 'day_of_week' to store the numeric day of the week
joined_data$day_of_week_numeric <- sapply(weekdays(joined_data$date), function(x) {
  match(x, c("Saturday", "Sunday", "Monday", "Tuesday", "Wednesday", "Thursday", "Friday"))
})

summary(joined_data$day_of_week_numeric)
```

Add a feature for how many purchases the user has made at the point of that purchase
```{r}
# sort the data
joined_data <- joined_data %>% 
  arrange(userID, date)

# calculate purchasing sum
joined_data <- joined_data %>%
  group_by(userID) %>%
  mutate(user_total_purchases_up_to_date = cumsum(count)) %>%
  ungroup()

summary(joined_data$user_total_purchases_up_to_date)
```

Add a feature for how many purchases of that specific item have been made by a user
```{r}
# sort
joined_data <- joined_data %>%
  arrange(userID, itemID, date)

# calculate sum o fpurchases of specific item by user
joined_data <- joined_data %>%
  group_by(userID, itemID) %>%
  mutate(item_purchases_by_user = cumsum(count)) %>%
  ungroup()

summary(joined_data$item_purchases_by_user)
```


Add a feature for how many total purchases have happened for an item
```{r}
# sort
joined_data <- joined_data %>%
  arrange(itemID, date)

# sum of item purchases to a date
joined_data <- joined_data %>%
  group_by(itemID) %>%
  mutate(total_item_purchases_up_to_date = cumsum(count)) %>%
  ungroup()

summary(joined_data$total_item_purchases_up_to_date)
```

What is the users weekly purchase rate for an item? (consider turning this into a window)
```{r}
# Find the first recorded purchase date
first_purchase_date <- min(joined_data$date)

# Calculate the total weeks since the first recorded purchase for each row
joined_data$total_weeks_since_first_purchase <- as.numeric(difftime(joined_data$date, first_purchase_date, units = "weeks")) + 1

# find weeks with purchases for each user-item pair
joined_data <- joined_data %>%
  group_by(userID, itemID) %>%
  mutate(week_num = floor(as.numeric(difftime(date, first_purchase_date, units = "weeks")))) %>%
  mutate(purchase_week_flag = 1) %>%
  distinct(userID, itemID, week_num, .keep_all = TRUE) %>%
  ungroup()

# purchase frequency for each user-item pair
joined_data <- joined_data %>%
  group_by(userID, itemID) %>%
  mutate(purchase_rate = sum(purchase_week_flag) / max(total_weeks_since_first_purchase)) %>%
  ungroup()

# cleanup
joined_data <- select(joined_data, -week_num, -purchase_week_flag)

summary(joined_data$purchase_rate)
```

How many weeks since the purchase was last made? (this takes hours to complete)
```{r}
# sort the data
joined_data <- joined_data %>%
  arrange(userID, itemID, date)

# weeks since last purchase for user-item was made
joined_data <- joined_data %>%
  group_by(userID, itemID) %>%
  mutate(days_since_last_user_item_purchase = difftime(date, lag(date, default = first(date)), units = "days"),
         weeks_since_last_user_item_purchase = days_since_last_user_item_purchase / 7) %>%
  ungroup()

# replace NA
joined_data$weeks_since_last_user_item_purchase[is.na(joined_data$weeks_since_last_user_item_purchase) | joined_data$weeks_since_last_user_item_purchase == 0] <- 30

# time difference should be weeks
joined_data$weeks_since_last_user_item_purchase <- as.numeric(joined_data$weeks_since_last_user_item_purchase)

summary(joined_data$weeks_since_last_user_item_purchase)
```

Has the user ever bought an item in a weekly fashion?
(LIBRARY ZOO NEEDED)
(DON'T USE THIS ONE, DELETES ALL MY ROWS)
```{r}
# sort
joined_data <- joined_data %>% 
  arrange(userID, itemID, date)

# Calculate the week number for each purchase relative to the first purchase in the dataset
joined_data <- joined_data %>%
  mutate(week_num = as.integer(floor(difftime(date, min(date, na.rm = TRUE), units = "weeks"))))

# Copy the original dataset to preserve all data
original_data <- joined_data

# Proceed with identifying weeks with at least one purchase
weekly_purchases <- joined_data %>%
  group_by(userID, itemID, week_num) %>%
  summarise(purchase_flag = 1, .groups = 'drop') # Note: This drops extra columns

# Complete the sequence for weeks and mark with purchase_flag
weekly_purchases <- weekly_purchases %>%
  group_by(userID, itemID) %>%
  complete(week_num = full_seq(c(min(week_num), max(week_num)), 1),
           fill = list(purchase_flag = 0))

# Calculate consecutive purchases
weekly_purchases <- weekly_purchases %>%
  mutate(consecutive_purchases = if_else(rollapply(purchase_flag, width = 3, by = 1, FUN = sum, fill = 0, align = "left") == 3, 1, 0))

# Flag users with three consecutive weeks of purchases
weekly_purchases <- weekly_purchases %>%
  group_by(userID, itemID) %>%
  summarise(three_consecutive_weeks = max(consecutive_purchases), .groups = 'drop')

# Join the flags back to the original dataset
joined_data <- original_data %>%
  left_join(weekly_purchases, by = c("userID", "itemID", "week_num"))

summary(joined_data$three_consecutive_weeks)
```


```{r}

```


### Trends in the Data

How many purchases are made by week?
```{r}
# Convert "date" column to Date type
joined_data$date <- as.Date(joined_data$date)

# Extract year and week number
joined_data$year <- year(joined_data$date)
joined_data$week <- week(joined_data$date)

# Combine year and week for unique identification across years
joined_data$year_week <- paste(joined_data$year, joined_data$week, sep="-")
```

```{r}
weekly_purchases <- joined_data %>%
  group_by(year_week) %>%
  summarise(total_purchases = n())

# View the first few rows to verify
head(weekly_purchases)
```

```{r}
ggplot(weekly_purchases, aes(x = year_week, y = total_purchases)) +
  geom_bar(stat = "identity", fill = "steelblue") +
  theme(axis.text.x = element_text(angle = 90, hjust = 1)) +
  labs(title = "Weekly Purchases Distribution",
       x = "Week (Year-Week Number)",
       y = "Total Purchases")

print(weekly_purchases)
```

How many test.csv purchases were never made in the training data?  
```{r}
# Create a unique identifier in both datasets by pasting userID and itemID
joined_data$uid_item_comb <- paste(joined_data$userID, joined_data$itemID, sep = "_")
test$uid_item_comb <- paste(test$userID, test$itemID, sep = "_")

# Identify combinations in test_data not present in joined_data
unique_combinations_not_in_joined <- test %>%
  filter(!(uid_item_comb %in% joined_data$uid_item_comb))

# Count the total number of unique rows
total_unique_not_in_joined <- nrow(unique_combinations_not_in_joined)

# Print the result
print(total_unique_not_in_joined)
```

What is the breakdown by day?
```{r}
# Extract the day of the week
joined_data$day_of_week <- weekdays(joined_data$date)
# Aggregate data
purchases_by_day <- joined_data %>%
  group_by(day_of_week) %>%
  summarise(total_purchases = n()) %>%
  mutate(day_of_week = factor(day_of_week, levels = c("Monday", "Tuesday", "Wednesday", "Thursday", "Friday", "Saturday", "Sunday")))

# View the result
print(purchases_by_day)
```

```{r}
# Plot
ggplot(purchases_by_day, aes(x = day_of_week, y = total_purchases, fill = day_of_week)) +
  geom_bar(stat = "identity", color = "black") +
  theme_minimal() +
  labs(title = "Distribution of Purchases Across Days of the Week",
       x = "Day of the Week",
       y = "Total Purchases") +
  theme(axis.text.x = element_text(angle = 45, hjust = 1), # Improve readability of x labels
        legend.title = element_blank()) # Remove the legend title
```

I will extract the target variable for the training data based on if the purchase was repeated in January. I have to change what this means though.  
Target variable: Was the purchase made again during a 4 week window in January?  
*January has higher purchases, this could be higher first time purchases or could be higher repeats from earlier in the year. I should answer this at some point.  
0 - no.
1 - first monday to sunday (1/4-1/10)
2 - and so on (1/11-1/17)
3 - (1/18 - 1/24)
4 - (1/25 - 1/31)

```{r}
training_data <- joined_data[joined_data$date < as.Date("2021-01-04"),]
#summary(training_data)
```

```{r}
# Filter orders for January 2021
jan_orders <- joined_data[joined_data$date >= as.Date("2021-01-04") & joined_data$date <= as.Date("2021-01-31"),]
#summary(jan_orders)
# Add week categories to January orders
jan_orders$week_category <- cut(jan_orders$date,
                                breaks = as.Date(c("2021-01-03", "2021-01-10", "2021-01-17", "2021-01-24", "2021-01-31")),
                                labels = c("1", "2", "3", "4"),
                                include.lowest = TRUE, 
                                right = TRUE)

# Convert 'week_category' from factors to numeric
jan_orders$week_category_numeric <- as.numeric(as.character(jan_orders$week_category))

# Now aggregate using the numeric week category
repeat_jan_orders_summary <- aggregate(week_category_numeric ~ userID + itemID, data = jan_orders, FUN = min)

# Optionally, if you want to keep the factor labels in the summary
repeat_jan_orders_summary$week_category <- as.factor(repeat_jan_orders_summary$week_category_numeric)

# Merge this summary back into your training data to create the target variable
training_data$target <- 0  # Default to no repeat orders in January
training_data <- merge(training_data, repeat_jan_orders_summary, by = c("userID", "itemID"), all.x = TRUE)
training_data$target[!is.na(training_data$week_category)] <- as.numeric(as.character(training_data$week_category[!is.na(training_data$week_category)]))
training_data$target[is.na(training_data$target)] <- 0
```

```{r}
summary(training_data$target)
training_data$target <- as.factor(training_data$target)
# Create the bar chart
ggplot(training_data, aes(x = target)) +
  geom_bar(fill = "steelblue", color = "black", aes(y = ..count..)) +  # This plots the bars
  geom_text(stat='count', aes(label=..count.., y=..count..), vjust=-0.5, color="black") + # This adds the count labels
  theme_minimal() +
  labs(title = "Distribution of Target Variable",
       x = "Target Category",
       y = "Count")
```

```{r}
# I wanted to see what the breakdown would be for the sample_submission which has a score of 0.18 out of 0.3
sample_submission <- read.csv("contest1data/sample_submission.csv")
summary(sample_submission)
```

I submitted all 0s and got 0.24294  

### Linear Model Training

Only want to train on some features
```{r}
# Example list of features you want to train on
selected_features <- c("userID", "itemID", "count", "manufacturerID", "f1", "f2", "f3", "f4", "f5", "day_of_week_numeric", "user_total_purchases_up_to_date", "item_purchases_by_user", "total_item_purchases_up_to_date", "total_weeks_since_first_purchase", "purchase_rate", "week_num", "target")

# Create a new dataframe with only the selected features
training_data_selected <- training_data[selected_features]

```


Optimize feature selection with LASSO
```{r}
# Create matrix of predictor variables (exclude the target variable)
predictors <- as.matrix(training_data_selected[, -which(names(training_data_selected) == "target")])

# Create a vector for the target variable
response <- training_data_selected$target
```

Fit LASSO model
```{r}
set.seed(10903866) # For reproducibility
lasso_model <- glmnet(predictors, response, alpha = 1, family = "multinomial")
```

Cross validation for optimal lamda
```{r}
cv_lasso <- cv.glmnet(predictors, response, alpha = 1, family = "multinomial")
```

Visualize optimal lamda and pick minimum lamda
```{r}
plot(cv_lasso)
best_lambda <- cv_lasso$lambda.min  # or lambda.1se for a more regularized model
```

Optimal lamda
```{r}
lasso_optimal <- glmnet(predictors, response, alpha = 1, family = "multinomial", lambda = best_lambda)
```

What are the coefficients
```{r}
coef(lasso_optimal)
```

### Predicting Using LASSO

```{r}
# joining items with test
test_expanded <- left_join(test, items, by = "itemID")
```

Preparing the test dataset
```{r}
# what is the newest date for each combination in joined_data?
latest_joined_data <- joined_data %>%
  group_by(userID, itemID) %>%
  filter(date == max(date)) %>%
  ungroup() %>%
  select(userID, itemID, date, total_weeks_since_first_purchase, everything())

# Identify columns in 'latest_joined_data' not in 'test_expanded'
columns_to_join <- setdiff(names(latest_joined_data), names(test_expanded))

# Ensure 'userID' and 'itemID' are included for the join condition
key_columns <- c("userID", "itemID")
join_columns <- c(key_columns, columns_to_join)

# Perform the join with selected columns
test_joined <- left_join(test_expanded, latest_joined_data[join_columns], by = key_columns)

# Set default for 'total_weeks_since_first_purchase'
test_joined$total_weeks_since_first_purchase[is.na(test_joined$total_weeks_since_first_purchase)] <- 28

# Set defaults for other newly added columns
new_columns <- setdiff(columns_to_join, key_columns)
for(col_name in new_columns) {
  if(col_name != "total_weeks_since_first_purchase") { # Assuming default is 0 for these
    test_joined[[col_name]][is.na(test_joined[[col_name]])] <- 0
  }
}

head(test_joined)
```

Predict!
```{r}
predictors_names <- colnames(predictors)  # or manually list them if you've kept track

# Select the predictors from 'test_joined'
test_predictors <- as.matrix(test_joined[predictors_names])

# If you want the predicted class directly
predictions_class <- predict(lasso_optimal, newx = test_predictors, type = "class")

test_joined$target <- as.numeric(predictions_class)  # Convert to numeric and adjust range if needed

# Summary statistics for the target column
summary(test_joined$target)
```
What do the predictions look like?
```{r}
ggplot(test_joined, aes(x = factor(target))) +
  geom_bar(fill = "steelblue") +
  labs(title = "Distribution of Predicted Classes", x = "Class", y = "Count")
```

Converting the resulting predictions to submission csv
```{r}
# Assuming 'ID' is the name of your identifier column in `test_joined`
submission <- test_joined %>% select(uid_item_comb, target)

submission <- test_joined %>%
  # Replace underscores with dashes in uid_item_comb
  mutate(uid_item_comb = gsub("_", "-", uid_item_comb)) %>%
  # Select and rename columns
  select(ID = uid_item_comb, target)

write.csv(submission, "submission.csv", row.names = FALSE)
```

### Cleaning items
Impute missing f values
```{r}
items$f1[items$f1 == -1] <- 0

# Calculate median for f3, excluding -1
median_f3 <- median(items$f3[items$f3 != -1], na.rm = TRUE)
# Replace -1 with median in f3
items$f3[items$f3 == -1] <- median_f3

# f4
median_f4 <- median(items$f4[items$f4 != -1], na.rm = TRUE)
items$f4[items$f4 == -1] <- median_f4

# f5
median_f5 <- median(items$f5[items$f5 != -1], na.rm = TRUE)
items$f5[items$f5 == -1] <- median_f5
```

Clean categories by only taking the first number in the list
```{r}
items <- items %>%
  mutate(category = str_split(category, ",", simplify = TRUE)[, 1])

# handle NA
items <- items %>%
  mutate(category = if_else(category == "", NA_character_, category))

# remove all [ ]
items <- items %>%
  mutate(category = str_remove_all(category, "\\[|\\]"))

head(items)
```

Impute missing categories
```{r}
# Rows where 'category' is NA
items_na <- items %>%
  filter(is.na(category))

# Rows where 'category' is not NA
items_not_na <- items %>%
  filter(!is.na(category))

impute_category <- function(row, source_df) {
  # Find a row in source_df that matches f1-f5 values of the input row
  match <- source_df %>%
    filter(f1 == row$f1, f2 == row$f2, f3 == row$f3, f4 == row$f4, f5 == row$f5) %>%
    slice(1)  # Take the first match if there are multiple
  
  # If a match is found, return its 'category'; otherwise, return NA
  if(nrow(match) > 0) {
    return(match$category[1])
  } else {
    return(NA)
  }
}

items_na_imputed <- items_na %>%
  rowwise() %>%
  mutate(category = impute_category(cur_data(), items_not_na)) %>%
  ungroup()

items_imputed <- bind_rows(items_na_imputed, items_not_na)

# Before imputation
sum(is.na(items$category))

# After imputation
sum(is.na(items_imputed$category))
```

Merge categories with orders
```{r}
orders_with_category <- left_join(orders, items %>% dplyr::select(itemID, category), by = "itemID")
head(orders_with_category)
```


### New Matrix Design
This matrix is made for dates earlier than 2021-01-04. Training on if the 
item/user combos are purchased in january.

Matrix for each unique itemID and userID combinations
```{r}
# get dates before the last 4 weeks in Jan
training_data <- orders_with_category[orders_with_category$date > as.Date("2020-12-01"),]

# Extract unique userID and itemID combinations
training_matrix <- training_data %>%
  distinct(userID, itemID, category)
```

```{r}
# Filter orders for January 2021
jan_orders <- orders_with_category[orders_with_category$date >= as.Date("2021-01-04") & orders_with_category$date <= as.Date("2021-01-31"),]
#summary(jan_orders)

# Add week categories to January orders
jan_orders$week_category <- cut(jan_orders$date,
                                breaks = as.Date(c("2021-01-03", "2021-01-10", "2021-01-17", "2021-01-24", "2021-01-31")),
                                labels = c("1", "2", "3", "4"),
                                include.lowest = TRUE, 
                                right = TRUE)

# Convert 'week_category' from factors to numeric
jan_orders$week_category_numeric <- as.numeric(as.character(jan_orders$week_category))

# Now aggregate using the numeric week category
repeat_jan_orders_summary <- aggregate(week_category_numeric ~ userID + itemID, data = jan_orders, FUN = min)

# Optionally, if you want to keep the factor labels in the summary
repeat_jan_orders_summary$week_category <- as.factor(repeat_jan_orders_summary$week_category_numeric)

# Merge this summary back into your training data to create the target variable
training_matrix$target <- 0  # Default to no repeat orders in January
training_matrix <- merge(training_matrix, repeat_jan_orders_summary, by = c("userID", "itemID"), all.x = TRUE)
training_matrix$target[!is.na(training_matrix$week_category)] <- as.numeric(as.character(training_matrix$week_category[!is.na(training_matrix$week_category)]))
training_matrix$target[is.na(training_matrix$target)] <- 0
```

```{r}
summary(training_matrix$target)
training_matrix$target <- as.factor(training_matrix$target)
# Create the bar chart
ggplot(training_matrix, aes(x = target)) +
  geom_bar(fill = "steelblue", color = "black", aes(y = ..count..)) +  # This plots the bars
  geom_text(stat='count', aes(label=..count.., y=..count..), vjust=-0.5, color="black") + # This adds the count labels
  theme_minimal() +
  labs(title = "Distribution of Target Variable",
       x = "Target Category",
       y = "Count")
```

how many of an item has the user bought?
```{r}
# calculate total purchases
total_purchases <- training_data %>%
  group_by(userID, itemID) %>%
  summarise(total_count = sum(count, na.rm = TRUE)) %>%
  ungroup()

# group back into training
training_matrix <- left_join(training_matrix, total_purchases, by = c("userID", "itemID"))

# handle any na's
training_matrix$total_count[is.na(training_matrix$total_count)] <- 0

summary(training_matrix$total_count)
```

What proportion of their purchases has the user bought of items in this category?
```{r}
total_purchases_by_user <- orders_with_category %>%
  group_by(userID) %>%
  summarise(total_purchases = n()) %>%
  ungroup()

purchases_by_user_category <- orders_with_category %>%
  group_by(userID, category) %>%
  summarise(category_purchases = n()) %>%
  ungroup()

category_ratios <- left_join(purchases_by_user_category, total_purchases_by_user, by = "userID")

category_ratios <- category_ratios %>%
  mutate(purchase_ratio = category_purchases / total_purchases)

# category_features_for_modeling <- category_ratios %>%
#   pivot_wider(names_from = category, values_from = purchase_ratio, values_fill = list(purchase_ratio = 0)) %>%
#   ungroup()
training_matrix <- left_join(training_matrix, category_ratios, by = c("userID", "category"))

summary(training_matrix$purchase_ratio)
```



how many weeks has the user made this purchase?
```{r}
# week num
training_data <- training_data %>%
  mutate(date = as.Date(date), # Ensure date is in Date format
         week_num = isoweek(date)) # Extract ISO week number

# unique purchase weeks
user_item_weeks <- training_data %>%
  group_by(userID, itemID) %>%
  summarise(unique_purchase_weeks = n_distinct(week_num)) %>%
  ungroup()

# join and handle na's
training_matrix <- left_join(training_matrix, user_item_weeks, by = c("userID", "itemID"))
training_matrix$unique_purchase_weeks[is.na(training_matrix$unique_purchase_weeks)] <- 0

summary(training_matrix$unique_purchase_weeks)
```

What was the latest purchase of this item by the user? (change the latest date depending on training data against January or including Jan.)
```{r}
# Define the reference date
reference_date <- as.Date("2021-01-03")

# Find the most recent purchase date for each user-item combination
recent_purchases <- training_data %>%
  group_by(userID, itemID) %>%
  summarise(most_recent_purchase = max(date)) %>%
  ungroup()

# weeks since recent purchase
recent_purchases <- recent_purchases %>%
  mutate(weeks_since_last_purchase = as.numeric(difftime(reference_date, most_recent_purchase, units = "weeks")))

# join into training matrix
training_matrix <- left_join(training_matrix, recent_purchases, by = c("userID", "itemID"))

# Replace NA values with 40
training_matrix$weeks_since_last_purchase[is.na(training_matrix$weeks_since_last_purchase)] <- 40

# Optionally remove the 'most_recent_purchase' column if it's no longer needed
training_matrix <- dplyr::select(training_matrix, -most_recent_purchase)

summary(training_matrix$weeks_since_last_purchase)
```

What is the average time between purchases of this item by the user?
```{r}
# sort
training_data <- training_data %>%
  arrange(userID, itemID, date)

# Calculate the time difference in weeks between purchases
training_data <- training_data %>%
  group_by(userID, itemID) %>%
  mutate(time_diff = c(NA, diff(date) / 7)) %>%
  ungroup() # Ensure to ungroup after mutation
  # group_by(userID, itemID) %>%
  # mutate(weeks_between_purchases = difftime(date, lag(date), units = "weeks")) %>%
  # ungroup()

# Calculate the average time in weeks between purchases
average_weeks_between_purchases <- training_data %>%
  group_by(userID, itemID) %>%
  summarise(avg_time_between_purchases = mean(time_diff, na.rm = TRUE)) %>%
  ungroup()
  # group_by(userID, itemID) %>%
  # summarise(average_weeks = mean(weeks_between_purchases, na.rm = TRUE)) %>%
  # ungroup()

# replace single purchases and NAs
average_weeks_between_purchases <- average_weeks_between_purchases %>%
  mutate(avg_time_between_purchases = ifelse(is.na(avg_time_between_purchases) | avg_time_between_purchases == 0, 40, avg_time_between_purchases))

# back into the training matrix
training_matrix <- left_join(training_matrix, average_weeks_between_purchases, by = c("userID", "itemID"))

# handling nas after joining
#training_matrix$avg_time_between_purchases[is.na(training_matrix$avg_time_between_purchases)] <- 40

# Assign 40 to user-item pairs with no purchases (thus no row in average_weeks_between_purchases and resulted in NA after the join)
summary(training_matrix$avg_time_between_purchases)
```

Has the user bought this item more than once?
```{r}
# Count purchases per User-Item Combo
purchase_counts <- training_data %>%
  group_by(userID, itemID) %>%
  summarise(purchase_count = n()) %>%
  ungroup()

# binary indicator for multiple purchases
purchase_counts <- purchase_counts %>%
  mutate(multiple_purchases = if_else(purchase_count > 1, 1, 0))

# join back
training_matrix <- left_join(training_matrix, purchase_counts %>% dplyr::select(userID, itemID, multiple_purchases), by = c("userID", "itemID"))

# handle NAs
training_matrix$multiple_purchases[is.na(training_matrix$multiple_purchases)] <- 0

summary(training_matrix$multiple_purchases)

```

Has the user bought this item in December?
```{r}
# Filter for purchases made in December
december_purchases <- orders %>%
  filter(month(date) == 12) %>%
  distinct(userID, itemID) # Get unique user-item pairs for December

# Add a binary indicator for December purchases
december_purchases <- december_purchases %>%
  mutate(purchased_in_december = 1)

training_matrix <- left_join(training_matrix, december_purchases %>% dplyr::select(userID, itemID, purchased_in_december), by = c("userID", "itemID"))

training_matrix$purchased_in_december[is.na(training_matrix$purchased_in_december)] <- 0

summary(training_matrix$purchased_in_december)
```

Is this item associated with repeat purchases?
```{r}
# Calculate the number of purchases for each user-item combination
user_item_purchases <- training_data %>%
  group_by(userID, itemID) %>%
  summarise(purchase_count = n()) %>%
  ungroup()

# Determine if each purchase was a repeat purchase (more than once)
user_item_purchases <- user_item_purchases %>%
  mutate(repeat_purchase = if_else(purchase_count > 1, 1, 0))

# Calculate repeat purchase frequency for each item
item_repeat_frequency <- user_item_purchases %>%
  group_by(itemID) %>%
  summarise(repeat_purchase_score = sum(repeat_purchase) / n()) %>%
  ungroup()

# join back
training_matrix <- left_join(training_matrix, item_repeat_frequency, by = "itemID")

training_matrix$repeat_purchase_score[is.na(training_matrix$repeat_purchase_score)] <- 0

summary(training_matrix$repeat_purchase_score)
```

What is the linear trend in popularity of the item?
```{r}
# prep
training_data <- training_data %>%
  mutate(date = as.Date(date, format = "%Y-%m-%d"),
         year_month = format(date, "%Y-%m"))

# count purchases by item and month
monthly_purchases <- training_data %>%
  group_by(itemID, year_month) %>%
  summarise(purchase_count = n(), .groups = 'drop')

# Assuming the date column is already in the correct format
monthly_purchases <- monthly_purchases %>%
  mutate(month_index = as.Date(paste0(year_month, "-01")),
         time_index = as.numeric(difftime(month_index, min(month_index), units = "days")) / 30)  # Approximate to convert days to months

# fit linear model for each item
item_trends <- monthly_purchases %>%
  group_by(itemID) %>%
  do(tidy(lm(purchase_count ~ time_index, data = .))) %>%
  ungroup()

# interpret slope info
slope_info <- item_trends %>%
  filter(term == "time_index") %>%
  dplyr::select(itemID, estimate)

# slope info back into data frame
training_matrix <- left_join(training_matrix, slope_info, by = "itemID")
training_matrix$estimate[is.na(training_matrix$estimate)] <- 0


summary(training_matrix$estimate)
```

How long ago has the user bought anything?
Specific part of the month for purchases (item, user, item+user?)
```{r}

```

#### Do we want to balance the matrix target 0 values?
```{r}
# Rows where target is not 0
rows_with_nonzero_target <- training_matrix %>%
  filter(target != 0)

# Rows where target is 0
rows_with_zero_target <- training_matrix %>%
  filter(target == 0)

# Check if the number of rows is greater than some value
if(nrow(rows_with_zero_target) > 40000) {
  rows_with_zero_target_sampled <- sample_n(rows_with_zero_target, 40000)
} else {
  rows_with_zero_target_sampled <- rows_with_zero_target
}

# combine back rows
training_matrix_updated <- bind_rows(rows_with_nonzero_target, rows_with_zero_target_sampled)

# verify
training_matrix_updated$target <- as.factor(training_matrix_updated$target)
# Create the bar chart
ggplot(training_matrix_updated, aes(x = target)) +
  geom_bar(fill = "steelblue", color = "black", aes(y = ..count..)) +  # This plots the bars
  geom_text(stat='count', aes(label=..count.., y=..count..), vjust=-0.5, color="black") + # This adds the count labels
  theme_minimal() +
  labs(title = "Distribution of Target Variable",
       x = "Target Category",
       y = "Count")
```


### LASSO Training

Only want to train on some features
```{r}
# Example list of features you want to train on
selected_features <- c("userID", "itemID", "target", "total_count", "unique_purchase_weeks", "weeks_since_last_purchase", "avg_time_between_purchases", "multiple_purchases", "purchased_in_december")

# Create a new dataframe with only the selected features
training_data_selected <- training_matrix[selected_features]

```


Optimize feature selection with LASSO
```{r}
# Create matrix of predictor variables (exclude the target variable)
predictors <- dplyr::select(training_data_selected, -target) %>% as.matrix()

# Create a vector for the target variable
response <- training_data_selected$target
#str(training_data_selected$target)
#summary(training_data_selected$target)
response <- factor(response, levels = c(0, 1, 2, 3, 4))  # Adjust levels accordingly
```

Cross validation for optimal lamda
```{r}
set.seed(10903866) 
cv_lasso <- cv.glmnet(predictors, response, alpha = 1, family = "multinomial")
```

Visualize optimal lamda and pick minimum lamda
```{r}
plot(cv_lasso)
best_lambda <- cv_lasso$lambda.min  # or lambda.1se for a more regularized model
```

Optimal lamda
```{r}
lasso_optimal <- glmnet(predictors, response, alpha = 1, family = "multinomial", lambda = cv_lasso$lambda.1se, maxit = 200000)
```

```{r}
lasso_optimal <- cv.glmnet(predictors, response, alpha = 1, family = "multinomial", lambda = "lambda.min")
```


What are the coefficients
```{r}
coef(lasso_optimal)
```

### Predict Using LASSO and New Matrix

Preparing the test dataset
```{r}
# test needs to be joined with the matrix that is formed on January data as well...
```

Predict!
```{r}
predictors_names <- colnames(predictors)  # or manually list them if you've kept track

# Select the predictors from 'test_joined'
test_predictors <- as.matrix(test_joined[predictors_names])

# If you want the predicted class directly
predictions_class <- predict(lasso_optimal, newx = test_predictors, type = "class")

test_joined$target <- as.numeric(predictions_class)  # Convert to numeric and adjust range if needed

# Summary statistics for the target column
summary(test_joined$target)
```
What do the predictions look like?
```{r}
ggplot(test_joined, aes(x = factor(target))) +
  geom_bar(fill = "steelblue") +
  labs(title = "Distribution of Predicted Classes", x = "Class", y = "Count")
```

Converting the resulting predictions to submission csv
```{r}
# Assuming 'ID' is the name of your identifier column in `test_joined`
submission <- test_joined %>% select(uid_item_comb, target)

submission <- test_joined %>%
  # Replace underscores with dashes in uid_item_comb
  mutate(uid_item_comb = gsub("_", "-", uid_item_comb)) %>%
  # Select and rename columns
  select(ID = uid_item_comb, target)

write.csv(submission, "submission.csv", row.names = FALSE)
```


### Asking some questions for data engineering
How many purchases in test exist during January?
```{r}
# Filter for January purchases
january_orders <- orders %>%
  filter(month(date) == 1)

january_combinations <- january_orders %>%
  distinct(userID, itemID)

# Add a flag to indicate presence in January orders
test_with_january_flag <- test %>%
  mutate(in_january = ifelse(paste(userID, itemID) %in% paste(january_combinations$userID, january_combinations$itemID), 1, 0))

percentage_in_january <- mean(test_with_january_flag$in_january) * 100

print(paste("Percentage of test combinations present in December orders:", percentage_in_january, "%"))

```

```{r}
percentages <- c()
```

How many of the proposed orders are repeated multiple times?
```{r}
purchase_frequencies <- orders %>%
  group_by(userID, itemID) %>%
  summarise(purchase_count = n()) %>%
  ungroup()

purchase_frequencies <- purchase_frequencies %>%
  mutate(multiple_purchases = if_else(purchase_count > 1, 1, 0))

test_with_purchase_info <- left_join(test, purchase_frequencies, by = c("userID", "itemID"))

percentage_multiple_purchases <- mean(test_with_purchase_info$multiple_purchases, na.rm = TRUE) * 100

print(paste("Percentage of test orders purchased multiple times:", percentage_multiple_purchases, "%"))

ggplot(purchase_frequencies, aes(x = factor(multiple_purchases))) +
  geom_bar(aes(y = ..prop.., group = 1), stat = "count", fill = "steelblue") +
  scale_y_continuous(labels = scales::percent) +
  labs(title = "Distribution of Multiple Purchases",
       x = "Multiple Purchases (0 = Single, 1 = Multiple)",
       y = "Percentage of Purchases") +
  theme_minimal()
```
What is the average times purchased
```{r}
total_purchases_per_combination <- orders %>%
  group_by(userID, itemID) %>%
  summarise(total_count = sum(count, na.rm = TRUE)) %>%
  ungroup()

average_purchases <- mean(total_purchases_per_combination$total_count, na.rm = TRUE)

print(paste("Average amount of purchases per user-item combination:", average_purchases))
```

### Random Forest Training

Select the Predictor Variables and the Target Variable
```{r}
# Define the columns to use as predictors
predictor_columns <- c("weeks_since_last_purchase", "repeat_purchase_score", "estimate", "purchase_ratio")

# Extract predictors and the target variable
predictors <- training_matrix_updated[, predictor_columns]
response <- training_matrix_updated$target
```

Split the Data into Training and Test Sets
```{r}
set.seed(111)  # For reproducibility
sample_size <- floor(0.7 * nrow(training_matrix_updated))
train_indices <- sample(seq_len(nrow(training_matrix_updated)), size = sample_size)

train_predictors <- predictors[train_indices, ]
train_response <- response[train_indices]

test_predictors <- predictors[-train_indices, ]
test_response <- response[-train_indices]
```

Train the Random Forest Model
```{r}
rf_model <- randomForest(x = train_predictors, y = train_response, ntree = 500)
```

Make Predictions and Evaluate the Model
```{r}
predictions <- predict(rf_model, newdata = test_predictors)

# Evaluate model performance
confusionMatrix <- table(test_response, predictions)
print(confusionMatrix)

# Calculate accuracy
accuracy <- sum(diag(confusionMatrix)) / sum(confusionMatrix)
print(paste("Accuracy:", accuracy))
```

```{r}
# Assuming rf_model is your trained Random Forest model
importance_values <- importance(rf_model)

# Convert to a data frame for plotting
feature_importance <- data.frame(Feature = rownames(importance_values), Importance = importance_values[, "MeanDecreaseGini"])

# Plotting feature importance
ggplot(feature_importance, aes(x = reorder(Feature, Importance), y = Importance)) +
  geom_bar(stat = "identity") +
  coord_flip() +  # Flip coordinates for horizontal layout
  labs(title = "Feature Importance (Mean Decrease Gini)", x = "Feature", y = "Mean Decrease in Gini Impurity") +
  theme_minimal()
```

```{r}
tree_1 <- getTree(rf_model, k = 1, labelVar = TRUE)
```


Fine-Tuning and Cross-Validation
```{r}

```


#### Generate the test matrix
```{r}
# add category to test
test <- left_join(test, items %>% dplyr::select(itemID, category), by = "itemID")

```

how many of an item has the user bought?
```{r}
# calculate total purchases
total_purchases <- orders_with_category %>%
  group_by(userID, itemID) %>%
  summarise(total_count = sum(count, na.rm = TRUE)) %>%
  ungroup()

# group back into training
test <- left_join(test, total_purchases, by = c("userID", "itemID"))

# handle any na's
test$total_count[is.na(test$total_count)] <- 0

summary(test$total_count)
```

What proportion of their purchases has the user bought of items in this category?
```{r}
total_purchases_by_user <- orders_with_category %>%
  group_by(userID) %>%
  summarise(total_purchases = n()) %>%
  ungroup()

purchases_by_user_category <- orders_with_category %>%
  group_by(userID, category) %>%
  summarise(category_purchases = n()) %>%
  ungroup()

category_ratios <- left_join(purchases_by_user_category, total_purchases_by_user, by = "userID")

category_ratios <- category_ratios %>%
  mutate(purchase_ratio = category_purchases / total_purchases)

# category_features_for_modeling <- category_ratios %>%
#   pivot_wider(names_from = category, values_from = purchase_ratio, values_fill = list(purchase_ratio = 0)) %>%
#   ungroup()
test <- left_join(test, category_ratios, by = c("userID", "category"))

test$purchase_ratio[is.na(test$purchase_ratio)] <- 0

summary(test$purchase_ratio)
```

how many weeks has the user made this purchase?
```{r}
# week num
orders_with_category <- orders_with_category %>%
  mutate(date = as.Date(date), # Ensure date is in Date format
         week_num = isoweek(date)) # Extract ISO week number

# unique purchase weeks
user_item_weeks <- orders_with_category %>%
  group_by(userID, itemID) %>%
  summarise(unique_purchase_weeks = n_distinct(week_num)) %>%
  ungroup()

# join and handle na's
test <- left_join(test, user_item_weeks, by = c("userID", "itemID"))
test$unique_purchase_weeks[is.na(test$unique_purchase_weeks)] <- 0

summary(test$unique_purchase_weeks)
```

What was the latest purchase of this item by the user? (change the latest date depending on training data against January or including Jan.)
```{r}
# Define the reference date
reference_date <- as.Date("2021-01-31")

# Find the most recent purchase date for each user-item combination
recent_purchases <- orders_with_category %>%
  group_by(userID, itemID) %>%
  summarise(most_recent_purchase = max(date)) %>%
  ungroup()

# weeks since recent purchase
recent_purchases <- recent_purchases %>%
  mutate(weeks_since_last_purchase = as.numeric(difftime(reference_date, most_recent_purchase, units = "weeks")))

# join into training matrix
test <- left_join(test, recent_purchases, by = c("userID", "itemID"))

# Replace NA values with 40
test$weeks_since_last_purchase[is.na(test$weeks_since_last_purchase)] <- 40

# Optionally remove the 'most_recent_purchase' column if it's no longer needed
test <- dplyr::select(test, -most_recent_purchase)

summary(test$weeks_since_last_purchase)
```

What is the average time between purchases of this item by the user?
```{r}
# sort
orders_with_category <- orders_with_category %>%
  arrange(userID, itemID, date)

# Calculate the time difference in weeks between purchases
orders_with_category <- orders_with_category %>%
  group_by(userID, itemID) %>%
  mutate(time_diff = c(NA, diff(date) / 7)) %>%
  ungroup() # Ensure to ungroup after mutation
  # group_by(userID, itemID) %>%
  # mutate(weeks_between_purchases = difftime(date, lag(date), units = "weeks")) %>%
  # ungroup()

# Calculate the average time in weeks between purchases
average_weeks_between_purchases <- orders_with_category %>%
  group_by(userID, itemID) %>%
  summarise(avg_time_between_purchases = mean(time_diff, na.rm = TRUE)) %>%
  ungroup()
  # group_by(userID, itemID) %>%
  # summarise(average_weeks = mean(weeks_between_purchases, na.rm = TRUE)) %>%
  # ungroup()

# replace single purchases and NAs
average_weeks_between_purchases <- average_weeks_between_purchases %>%
  mutate(avg_time_between_purchases = ifelse(is.na(avg_time_between_purchases) | avg_time_between_purchases == 0, 40, avg_time_between_purchases))

# back into the training matrix
test <- left_join(test, average_weeks_between_purchases, by = c("userID", "itemID"))

# handling nas after joining
test$avg_time_between_purchases[is.na(test$avg_time_between_purchases)] <- 40

# Assign 40 to user-item pairs with no purchases (thus no row in average_weeks_between_purchases and resulted in NA after the join)
summary(test$avg_time_between_purchases)
```

Has the user bought this item more than once?
```{r}
# Count purchases per User-Item Combo
purchase_counts <- orders_with_category %>%
  group_by(userID, itemID) %>%
  summarise(purchase_count = n()) %>%
  ungroup()

# binary indicator for multiple purchases
purchase_counts <- purchase_counts %>%
  mutate(multiple_purchases = if_else(purchase_count > 1, 1, 0))

# join back
test <- left_join(test, purchase_counts %>% dplyr::select(userID, itemID, multiple_purchases), by = c("userID", "itemID"))

# handle NAs
test$multiple_purchases[is.na(test$multiple_purchases)] <- 0

summary(test$multiple_purchases)

```

Has the user bought this item in January?
```{r}
# Filter for purchases made in January
december_purchases <- orders_with_category %>%
  filter(month(date) == 1) %>%
  distinct(userID, itemID) # Get unique user-item pairs for January

# Add a binary indicator for January purchases
december_purchases <- december_purchases %>%
  mutate(purchased_in_december = 1)

test <- left_join(test, december_purchases %>% dplyr::select(userID, itemID, purchased_in_december), by = c("userID", "itemID"))

test$purchased_in_december[is.na(test$purchased_in_december)] <- 0

summary(test$purchased_in_december)
```

Is this item associated with repeat purchases?
```{r}
# Calculate the number of purchases for each user-item combination
user_item_purchases <- orders_with_category %>%
  group_by(userID, itemID) %>%
  summarise(purchase_count = n()) %>%
  ungroup()

# Determine if each purchase was a repeat purchase (more than once)
user_item_purchases <- user_item_purchases %>%
  mutate(repeat_purchase = if_else(purchase_count > 1, 1, 0))

# Calculate repeat purchase frequency for each item
item_repeat_frequency <- user_item_purchases %>%
  group_by(itemID) %>%
  summarise(repeat_purchase_score = sum(repeat_purchase) / n()) %>%
  ungroup()

# join back
test <- left_join(test, item_repeat_frequency, by = "itemID")

test$repeat_purchase_score[is.na(test$repeat_purchase_score)] <- 0

summary(test$repeat_purchase_score)
```

What is the linear trend in popularity of the item?
```{r}
# prep
orders_with_category <- orders_with_category %>%
  mutate(date = as.Date(date, format = "%Y-%m-%d"),
         year_month = format(date, "%Y-%m"))

# count purchases by item and month
monthly_purchases <- orders_with_category %>%
  group_by(itemID, year_month) %>%
  summarise(purchase_count = n(), .groups = 'drop')

# Assuming the date column is already in the correct format
monthly_purchases <- monthly_purchases %>%
  mutate(month_index = as.Date(paste0(year_month, "-01")),
         time_index = as.numeric(difftime(month_index, min(month_index), units = "days")) / 30)  # Approximate to convert days to months

# fit linear model for each item
item_trends <- monthly_purchases %>%
  group_by(itemID) %>%
  do(tidy(lm(purchase_count ~ time_index, data = .))) %>%
  ungroup()

# interpret slope info
slope_info <- item_trends %>%
  filter(term == "time_index") %>%
  dplyr::select(itemID, estimate)

# slope info back into data frame
test <- left_join(test, slope_info, by = "itemID")
test$estimate[is.na(test$estimate)] <- 0


summary(test$estimate)
```

And now we predict!
```{r}
colSums(is.na(test))
test$category[is.na(test$category)] <- 0
test$category_purchases[is.na(test$category_purchases)] <- 0
test$total_purchases[is.na(test$total_purchases)] <- 0

predictions_for_submission <- predict(rf_model, newdata = test)
```

```{r}
predictions_df <- data.frame(Predictions = factor(predictions_for_submission))
ggplot(predictions_df, aes(x = Predictions)) +
  geom_bar(fill = "steelblue") +
  labs(title = "Distribution of Test Predictions", x = "Predicted Class", y = "Count") +
  theme_minimal()
```

```{r}
submission <- data.frame(ID = test$ID, Target = predictions_for_submission)
write.csv(submission, "submission.csv", row.names = FALSE)

```

