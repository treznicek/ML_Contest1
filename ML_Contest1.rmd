---
title: "Competition 1"
author: "Tim Reznicek"
date: "2024-3-15"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r, echo=FALSE}
# Libraries
if (!requireNamespace("dplyr", quietly = TRUE)) install.packages("dplyr")
if (!require(glmnet)) install.packages("glmnet")
library(glmnet)
library(igraph)
library(plotly)
library(dplyr)
library(lubridate)
library(ggplot2)
library(zoo)
library(tidyr)
```

```{r}
# orders dataframe
orders <- read.csv("contest1data/orders.csv")
orders$date <- as.Date(orders$date)      # Making sure this data is date format
str(orders)
summary(orders)
```

```{r}
# items dataframe
items <- read.csv("contest1data/items.csv")
str(items)

```

```{r}
# categories dataframe
categories <- read.csv("contest1data/categories.csv", stringsAsFactors = FALSE)
str(categories)

```

```{r}
# test dataframe
test <- read.csv("contest1data/test.csv")
str(test)
summary(test)
```


```{r}
# how many manufacturerIDs exist, what is the most common?
summary(items$manufacturerID)
mode(items$manufacturerID)
```

```{r}
# Excluding empty values, summary of f columns
sapply(items[, c("f1", "f2", "f3", "f4", "f5")], function(x) summary(x[x != -1]))
```

```{r}
# how many missing values exist in each column?
missing_counts <- sapply(items, function(x) sum(is.na(x) | x == -1 | x == ""))
print(missing_counts)
```

```{r}
# missingness relation between features
with(items, table(category == "", f5 == -1))
with(items, table(category == "", f4 == -1))
with(items, table(category == "", f3 == -1))
with(items, table(f3 == -1, f5 == -1))
with(items, table(f4 == -1, f5 == -1))
with(items, table(f4 == -1, f3 == -1))
```

```{r}
# category column
cleaned_categories <- gsub("]", "", as.character(items$category[!is.na(items$category) & items$category != ""]))
# Split the cleaned categories into individual categories
categories <- strsplit(cleaned_categories, ",")
# Now, you can proceed to find unique categories and count them as before
unique_categories <- unique(unlist(categories))
category_counts <- table(unlist(categories))

# Print the counts of each category
#print(category_counts)

print("could reorder this to show the most represented at the top or pie chart or something.")
```

```{r}
# g <- graph_from_data_frame(d = categories, directed = TRUE)
# pdf("category_hierarchy.pdf", width = 50, height = 10)
# plot(g, layout = layout_as_tree(g), edge.arrow.size = 0.5, vertex.label = V(g)$name, vertex.size = 10, vertex.label.cex = 0.4)
# dev.off()
```

```{r}
# # Assuming 'g' is your igraph object
# coords <- layout_as_tree(g)  # Get coordinates for tree layout
# edge_x <- c(); edge_y <- c()
# for(e in E(g)){
#   # Extract the vertex ids for the ends of each edge
#   ends_ids <- ends(g, e, names = FALSE)
#   
#   # Use these ids to index into coords directly
#   edge_x <- c(edge_x, coords[ends_ids[1], 1], coords[ends_ids[2], 1], NA)
#   edge_y <- c(edge_y, coords[ends_ids[1], 2], coords[ends_ids[2], 2], NA)
# }
# 
# node_x <- coords[,1]
# node_y <- coords[,2]
# text_labels <- V(g)$name
# 
# # Create edges
# p <- plot_ly(type='scatter', mode='lines', x=~edge_x, y=~edge_y, line=list(color='black')) %>%
#   add_trace(x=~node_x, y=~node_y, mode='markers+text', text=~text_labels, textposition='bottom center', hoverinfo='text')
# 
# # Customize layout
# p <- layout(p, title='Category Hierarchy')
# 
# # Display the plot
# p
```


```{r}
items_filtered <- items
items_filtered[items_filtered == -1] <- NA  # Replace -1 with NA for filtering

# Select only the columns f1 to f5
features <- items_filtered[,c("f1", "f2", "f3", "f4", "f5")]

#scatterplots of all combinations of f1 to f5
pairs(features, panel = panel.smooth, main = "Scatterplots of Features f1 to f5")
```

Taking into account both the missingness relationships, how many values are missing in each feature, and the relationships between features, I have devised the following method to clean the features.  
f1: make them 0  
f2: no missing  
f3 and f4 have high overlap in missingness
f3: median (400 something)
f4: median (0)
f5: median as well, not sure if I can do any good predictions on this.  

What about categories? To start out I will consider missing categories to be empty. Eventually I would like to find features related to categories and potentially fill them in that way.  

Join orders and items on itemID.  

```{r}
# Join the datasets on 'itemID'
joined_data <- inner_join(orders, items, by = "itemID")
```

### Cleaning f1-f5
```{r}
joined_data$f1[joined_data$f1 == -1] <- 0

# Calculate median for f3, excluding -1
median_f3 <- median(joined_data$f3[joined_data$f3 != -1], na.rm = TRUE)
# Replace -1 with median in f3
joined_data$f3[joined_data$f3 == -1] <- median_f3

# f4
median_f4 <- median(joined_data$f4[joined_data$f4 != -1], na.rm = TRUE)
joined_data$f4[joined_data$f4 == -1] <- median_f4

# f5
median_f5 <- median(joined_data$f5[joined_data$f5 != -1], na.rm = TRUE)
joined_data$f5[joined_data$f5 == -1] <- median_f5
```


### Feature Extraction from Date:
```{r}
# create a new column 'day_of_week' to store the numeric day of the week
joined_data$day_of_week_numeric <- sapply(weekdays(joined_data$date), function(x) {
  match(x, c("Saturday", "Sunday", "Monday", "Tuesday", "Wednesday", "Thursday", "Friday"))
})

summary(joined_data$day_of_week_numeric)
```

Add a feature for how many purchases the user has made at the point of that purchase
```{r}
# sort the data
joined_data <- joined_data %>% 
  arrange(userID, date)

# calculate purchasing sum
joined_data <- joined_data %>%
  group_by(userID) %>%
  mutate(user_total_purchases_up_to_date = cumsum(count)) %>%
  ungroup()

summary(joined_data$user_total_purchases_up_to_date)
```

Add a feature for how many purchases of that specific item have been made by a user
```{r}
# sort
joined_data <- joined_data %>%
  arrange(userID, itemID, date)

# calculate sum o fpurchases of specific item by user
joined_data <- joined_data %>%
  group_by(userID, itemID) %>%
  mutate(item_purchases_by_user = cumsum(count)) %>%
  ungroup()

summary(joined_data$item_purchases_by_user)
```


Add a feature for how many total purchases have happened for an item
```{r}
# sort
joined_data <- joined_data %>%
  arrange(itemID, date)

# sum of item purchases to a date
joined_data <- joined_data %>%
  group_by(itemID) %>%
  mutate(total_item_purchases_up_to_date = cumsum(count)) %>%
  ungroup()

summary(joined_data$total_item_purchases_up_to_date)
```

What is the users weekly purchase rate for an item? (consider turning this into a window)
```{r}
# Find the first recorded purchase date
first_purchase_date <- min(joined_data$date)

# Calculate the total weeks since the first recorded purchase for each row
joined_data$total_weeks_since_first_purchase <- as.numeric(difftime(joined_data$date, first_purchase_date, units = "weeks")) + 1

# find weeks with purchases for each user-item pair
joined_data <- joined_data %>%
  group_by(userID, itemID) %>%
  mutate(week_num = floor(as.numeric(difftime(date, first_purchase_date, units = "weeks")))) %>%
  mutate(purchase_week_flag = 1) %>%
  distinct(userID, itemID, week_num, .keep_all = TRUE) %>%
  ungroup()

# purchase frequency for each user-item pair
joined_data <- joined_data %>%
  group_by(userID, itemID) %>%
  mutate(purchase_rate = sum(purchase_week_flag) / max(total_weeks_since_first_purchase)) %>%
  ungroup()

# cleanup
joined_data <- select(joined_data, -week_num, -purchase_week_flag)

summary(joined_data$purchase_rate)
```

How many weeks since the purchase was last made? (this takes hours to complete)
```{r}
# sort the data
joined_data <- joined_data %>%
  arrange(userID, itemID, date)

# weeks since last purchase for user-item was made
joined_data <- joined_data %>%
  group_by(userID, itemID) %>%
  mutate(days_since_last_user_item_purchase = difftime(date, lag(date, default = first(date)), units = "days"),
         weeks_since_last_user_item_purchase = days_since_last_user_item_purchase / 7) %>%
  ungroup()

# replace NA
joined_data$weeks_since_last_user_item_purchase[is.na(joined_data$weeks_since_last_user_item_purchase) | joined_data$weeks_since_last_user_item_purchase == 0] <- 30

# time difference should be weeks
joined_data$weeks_since_last_user_item_purchase <- as.numeric(joined_data$weeks_since_last_user_item_purchase)

summary(joined_data$weeks_since_last_user_item_purchase)
```

Has the user ever bought an item in a weekly fashion?
(LIBRARY ZOO NEEDED)
(DON'T USE THIS ONE, DELETES ALL MY ROWS)
```{r}
# sort
joined_data <- joined_data %>% 
  arrange(userID, itemID, date)

# Calculate the week number for each purchase relative to the first purchase in the dataset
joined_data <- joined_data %>%
  mutate(week_num = as.integer(floor(difftime(date, min(date, na.rm = TRUE), units = "weeks"))))

# Copy the original dataset to preserve all data
original_data <- joined_data

# Proceed with identifying weeks with at least one purchase
weekly_purchases <- joined_data %>%
  group_by(userID, itemID, week_num) %>%
  summarise(purchase_flag = 1, .groups = 'drop') # Note: This drops extra columns

# Complete the sequence for weeks and mark with purchase_flag
weekly_purchases <- weekly_purchases %>%
  group_by(userID, itemID) %>%
  complete(week_num = full_seq(c(min(week_num), max(week_num)), 1),
           fill = list(purchase_flag = 0))

# Calculate consecutive purchases
weekly_purchases <- weekly_purchases %>%
  mutate(consecutive_purchases = if_else(rollapply(purchase_flag, width = 3, by = 1, FUN = sum, fill = 0, align = "left") == 3, 1, 0))

# Flag users with three consecutive weeks of purchases
weekly_purchases <- weekly_purchases %>%
  group_by(userID, itemID) %>%
  summarise(three_consecutive_weeks = max(consecutive_purchases), .groups = 'drop')

# Join the flags back to the original dataset
joined_data <- original_data %>%
  left_join(weekly_purchases, by = c("userID", "itemID", "week_num"))

summary(joined_data$three_consecutive_weeks)
```


```{r}

```


### Trends in the Data

How many purchases are made by week?
```{r}
# Convert "date" column to Date type
joined_data$date <- as.Date(joined_data$date)

# Extract year and week number
joined_data$year <- year(joined_data$date)
joined_data$week <- week(joined_data$date)

# Combine year and week for unique identification across years
joined_data$year_week <- paste(joined_data$year, joined_data$week, sep="-")
```

```{r}
weekly_purchases <- joined_data %>%
  group_by(year_week) %>%
  summarise(total_purchases = n())

# View the first few rows to verify
head(weekly_purchases)
```

```{r}
ggplot(weekly_purchases, aes(x = year_week, y = total_purchases)) +
  geom_bar(stat = "identity", fill = "steelblue") +
  theme(axis.text.x = element_text(angle = 90, hjust = 1)) +
  labs(title = "Weekly Purchases Distribution",
       x = "Week (Year-Week Number)",
       y = "Total Purchases")

print(weekly_purchases)
```

How many test.csv purchases were never made in the training data?  
```{r}
# Create a unique identifier in both datasets by pasting userID and itemID
joined_data$uid_item_comb <- paste(joined_data$userID, joined_data$itemID, sep = "_")
test$uid_item_comb <- paste(test$userID, test$itemID, sep = "_")

# Identify combinations in test_data not present in joined_data
unique_combinations_not_in_joined <- test %>%
  filter(!(uid_item_comb %in% joined_data$uid_item_comb))

# Count the total number of unique rows
total_unique_not_in_joined <- nrow(unique_combinations_not_in_joined)

# Print the result
print(total_unique_not_in_joined)
```

What is the breakdown by day?
```{r}
# Extract the day of the week
joined_data$day_of_week <- weekdays(joined_data$date)
# Aggregate data
purchases_by_day <- joined_data %>%
  group_by(day_of_week) %>%
  summarise(total_purchases = n()) %>%
  mutate(day_of_week = factor(day_of_week, levels = c("Monday", "Tuesday", "Wednesday", "Thursday", "Friday", "Saturday", "Sunday")))

# View the result
print(purchases_by_day)
```

```{r}
# Plot
ggplot(purchases_by_day, aes(x = day_of_week, y = total_purchases, fill = day_of_week)) +
  geom_bar(stat = "identity", color = "black") +
  theme_minimal() +
  labs(title = "Distribution of Purchases Across Days of the Week",
       x = "Day of the Week",
       y = "Total Purchases") +
  theme(axis.text.x = element_text(angle = 45, hjust = 1), # Improve readability of x labels
        legend.title = element_blank()) # Remove the legend title
```

I will extract the target variable for the training data based on if the purchase was repeated in January. I have to change what this means though.  
Target variable: Was the purchase made again during a 4 week window in January?  
*January has higher purchases, this could be higher first time purchases or could be higher repeats from earlier in the year. I should answer this at some point.  
0 - no.
1 - first monday to sunday (1/4-1/10)
2 - and so on (1/11-1/17)
3 - (1/18 - 1/24)
4 - (1/25 - 1/31)

```{r}
training_data <- joined_data[joined_data$date < as.Date("2021-01-04"),]
#summary(training_data)
```

```{r}
# Filter orders for January 2021
jan_orders <- joined_data[joined_data$date >= as.Date("2021-01-04") & joined_data$date <= as.Date("2021-01-31"),]
#summary(jan_orders)
# Add week categories to January orders
jan_orders$week_category <- cut(jan_orders$date,
                                breaks = as.Date(c("2021-01-03", "2021-01-10", "2021-01-17", "2021-01-24", "2021-01-31")),
                                labels = c("1", "2", "3", "4"),
                                include.lowest = TRUE, 
                                right = TRUE)

# Convert 'week_category' from factors to numeric
jan_orders$week_category_numeric <- as.numeric(as.character(jan_orders$week_category))

# Now aggregate using the numeric week category
repeat_jan_orders_summary <- aggregate(week_category_numeric ~ userID + itemID, data = jan_orders, FUN = min)

# Optionally, if you want to keep the factor labels in the summary
repeat_jan_orders_summary$week_category <- as.factor(repeat_jan_orders_summary$week_category_numeric)

# Merge this summary back into your training data to create the target variable
training_data$target <- 0  # Default to no repeat orders in January
training_data <- merge(training_data, repeat_jan_orders_summary, by = c("userID", "itemID"), all.x = TRUE)
training_data$target[!is.na(training_data$week_category)] <- as.numeric(as.character(training_data$week_category[!is.na(training_data$week_category)]))
training_data$target[is.na(training_data$target)] <- 0
```

```{r}
summary(training_data$target)
training_data$target <- as.factor(training_data$target)
# Create the bar chart
ggplot(training_data, aes(x = target)) +
  geom_bar(fill = "steelblue", color = "black", aes(y = ..count..)) +  # This plots the bars
  geom_text(stat='count', aes(label=..count.., y=..count..), vjust=-0.5, color="black") + # This adds the count labels
  theme_minimal() +
  labs(title = "Distribution of Target Variable",
       x = "Target Category",
       y = "Count")
```

```{r}
# I wanted to see what the breakdown would be for the sample_submission which has a score of 0.18 out of 0.3
sample_submission <- read.csv("contest1data/sample_submission.csv")
summary(sample_submission)
```

I submitted all 0s and got 0.24294  

### Linear Model Training

Only want to train on some features
```{r}
# Example list of features you want to train on
selected_features <- c("userID", "itemID", "count", "manufacturerID", "f1", "f2", "f3", "f4", "f5", "day_of_week_numeric", "user_total_purchases_up_to_date", "item_purchases_by_user", "total_item_purchases_up_to_date", "total_weeks_since_first_purchase", "purchase_rate", "week_num", "target")

# Create a new dataframe with only the selected features
training_data_selected <- training_data[selected_features]

```


Optimize feature selection with LASSO
```{r}
# Create matrix of predictor variables (exclude the target variable)
predictors <- as.matrix(training_data_selected[, -which(names(training_data_selected) == "target")])

# Create a vector for the target variable
response <- training_data_selected$target
```

Fit LASSO model
```{r}
set.seed(10903866) # For reproducibility
lasso_model <- glmnet(predictors, response, alpha = 1, family = "multinomial")
```

Cross validation for optimal lamda
```{r}
cv_lasso <- cv.glmnet(predictors, response, alpha = 1, family = "multinomial")
```

Visualize optimal lamda and pick minimum lamda
```{r}
plot(cv_lasso)
best_lambda <- cv_lasso$lambda.min  # or lambda.1se for a more regularized model
```

Optimal lamda
```{r}
lasso_optimal <- glmnet(predictors, response, alpha = 1, family = "multinomial", lambda = best_lambda)
```

What are the coefficients
```{r}
coef(lasso_optimal)
```

### Predicting Using LASSO

```{r}
# joining items with test
test_expanded <- left_join(test, items, by = "itemID")
```

Preparing the test dataset
```{r}
# what is the newest date for each combination in joined_data?
latest_joined_data <- joined_data %>%
  group_by(userID, itemID) %>%
  filter(date == max(date)) %>%
  ungroup() %>%
  select(userID, itemID, date, total_weeks_since_first_purchase, everything())

# Identify columns in 'latest_joined_data' not in 'test_expanded'
columns_to_join <- setdiff(names(latest_joined_data), names(test_expanded))

# Ensure 'userID' and 'itemID' are included for the join condition
key_columns <- c("userID", "itemID")
join_columns <- c(key_columns, columns_to_join)

# Perform the join with selected columns
test_joined <- left_join(test_expanded, latest_joined_data[join_columns], by = key_columns)

# Set default for 'total_weeks_since_first_purchase'
test_joined$total_weeks_since_first_purchase[is.na(test_joined$total_weeks_since_first_purchase)] <- 28

# Set defaults for other newly added columns
new_columns <- setdiff(columns_to_join, key_columns)
for(col_name in new_columns) {
  if(col_name != "total_weeks_since_first_purchase") { # Assuming default is 0 for these
    test_joined[[col_name]][is.na(test_joined[[col_name]])] <- 0
  }
}

head(test_joined)
```

Predict!
```{r}
predictors_names <- colnames(predictors)  # or manually list them if you've kept track

# Select the predictors from 'test_joined'
test_predictors <- as.matrix(test_joined[predictors_names])

# If you want the predicted class directly
predictions_class <- predict(lasso_optimal, newx = test_predictors, type = "class")

test_joined$target <- as.numeric(predictions_class)  # Convert to numeric and adjust range if needed

# Summary statistics for the target column
summary(test_joined$target)
```
What do the predictions look like?
```{r}
ggplot(test_joined, aes(x = factor(target))) +
  geom_bar(fill = "steelblue") +
  labs(title = "Distribution of Predicted Classes", x = "Class", y = "Count")
```

Converting the resulting predictions to submission csv
```{r}
# Assuming 'ID' is the name of your identifier column in `test_joined`
submission <- test_joined %>% select(uid_item_comb, target)

submission <- test_joined %>%
  # Replace underscores with dashes in uid_item_comb
  mutate(uid_item_comb = gsub("_", "-", uid_item_comb)) %>%
  # Select and rename columns
  select(ID = uid_item_comb, target)

write.csv(submission, "submission.csv", row.names = FALSE)
```


### New Matrix Design
This matrix is made for dates earlier than 2021-01-04. Training on if the 
item/user combos are purchased in january.

Matrix for each unique itemID and userID combinations
```{r}
# get dates before the last 4 weeks in Jan
training_data <- orders[orders$date < as.Date("2021-01-04"),]

# Extract unique userID and itemID combinations
training_matrix <- training_data %>%
  distinct(userID, itemID)
```

```{r}
# Filter orders for January 2021
jan_orders <- orders[orders$date >= as.Date("2021-01-04") & orders$date <= as.Date("2021-01-31"),]
#summary(jan_orders)

# Add week categories to January orders
jan_orders$week_category <- cut(jan_orders$date,
                                breaks = as.Date(c("2021-01-03", "2021-01-10", "2021-01-17", "2021-01-24", "2021-01-31")),
                                labels = c("1", "2", "3", "4"),
                                include.lowest = TRUE, 
                                right = TRUE)

# Convert 'week_category' from factors to numeric
jan_orders$week_category_numeric <- as.numeric(as.character(jan_orders$week_category))

# Now aggregate using the numeric week category
repeat_jan_orders_summary <- aggregate(week_category_numeric ~ userID + itemID, data = jan_orders, FUN = min)

# Optionally, if you want to keep the factor labels in the summary
repeat_jan_orders_summary$week_category <- as.factor(repeat_jan_orders_summary$week_category_numeric)

# Merge this summary back into your training data to create the target variable
training_matrix$target <- 0  # Default to no repeat orders in January
training_matrix <- merge(training_matrix, repeat_jan_orders_summary, by = c("userID", "itemID"), all.x = TRUE)
training_matrix$target[!is.na(training_matrix$week_category)] <- as.numeric(as.character(training_matrix$week_category[!is.na(training_matrix$week_category)]))
training_matrix$target[is.na(training_matrix$target)] <- 0
```

```{r}
summary(training_matrix$target)
training_matrix$target <- as.factor(training_matrix$target)
# Create the bar chart
ggplot(training_matrix, aes(x = target)) +
  geom_bar(fill = "steelblue", color = "black", aes(y = ..count..)) +  # This plots the bars
  geom_text(stat='count', aes(label=..count.., y=..count..), vjust=-0.5, color="black") + # This adds the count labels
  theme_minimal() +
  labs(title = "Distribution of Target Variable",
       x = "Target Category",
       y = "Count")
```

how many of an item has the user bought?
```{r}
# calculate total purchases
total_purchases <- training_data %>%
  group_by(userID, itemID) %>%
  summarise(total_count = sum(count, na.rm = TRUE)) %>%
  ungroup()

# group back into training
training_matrix <- left_join(training_matrix, total_purchases, by = c("userID", "itemID"))

# handle any na's
training_matrix$total_count[is.na(training_matrix$total_count)] <- 0

summary(training_matrix$total_count)
```

how many weeks has the user made this purchase?
```{r}
# week num
training_data <- training_data %>%
  mutate(date = as.Date(date), # Ensure date is in Date format
         week_num = isoweek(date)) # Extract ISO week number

# unique purchase weeks
user_item_weeks <- training_data %>%
  group_by(userID, itemID) %>%
  summarise(unique_purchase_weeks = n_distinct(week_num)) %>%
  ungroup()

# join and handle na's
training_matrix <- left_join(training_matrix, user_item_weeks, by = c("userID", "itemID"))
training_matrix$unique_purchase_weeks[is.na(training_matrix$unique_purchase_weeks)] <- 0

summary(training_matrix$unique_purchase_weeks)
```

What was the latest purchase of this item by the user? (change the latest date depending on training data against January or including Jan.)
```{r}
# Define the reference date
reference_date <- as.Date("2021-01-03")

# Find the most recent purchase date for each user-item combination
recent_purchases <- training_data %>%
  group_by(userID, itemID) %>%
  summarise(most_recent_purchase = max(date)) %>%
  ungroup()

# weeks since recent purchase
recent_purchases <- recent_purchases %>%
  mutate(weeks_since_last_purchase = as.numeric(difftime(reference_date, most_recent_purchase, units = "weeks")))

# join into training matrix
training_matrix <- left_join(training_matrix, recent_purchases, by = c("userID", "itemID"))

# Replace NA values with 40
training_matrix$weeks_since_last_purchase[is.na(training_matrix$weeks_since_last_purchase)] <- 40

# Optionally remove the 'most_recent_purchase' column if it's no longer needed
training_matrix <- dplyr::select(training_matrix, -most_recent_purchase)

summary(training_matrix$weeks_since_last_purchase)
```

What is the average time between purchases of this item by the user?
```{r}
# sort
training_data <- training_data %>%
  arrange(userID, itemID, date)

# Calculate the time difference in weeks between purchases
training_data <- training_data %>%
  group_by(userID, itemID) %>%
  mutate(time_diff = c(NA, diff(date) / 7)) %>%
  ungroup() # Ensure to ungroup after mutation
  # group_by(userID, itemID) %>%
  # mutate(weeks_between_purchases = difftime(date, lag(date), units = "weeks")) %>%
  # ungroup()

# Calculate the average time in weeks between purchases
average_weeks_between_purchases <- training_data %>%
  group_by(userID, itemID) %>%
  summarise(avg_time_between_purchases = mean(time_diff, na.rm = TRUE)) %>%
  ungroup()
  # group_by(userID, itemID) %>%
  # summarise(average_weeks = mean(weeks_between_purchases, na.rm = TRUE)) %>%
  # ungroup()

# replace single purchases and NAs
average_weeks_between_purchases <- average_weeks_between_purchases %>%
  mutate(avg_time_between_purchases = ifelse(is.na(avg_time_between_purchases) | avg_time_between_purchases == 0, 40, avg_time_between_purchases))

# back into the training matrix
training_matrix <- left_join(training_matrix, average_weeks_between_purchases, by = c("userID", "itemID"))

# handling nas after joining
#training_matrix$avg_time_between_purchases[is.na(training_matrix$avg_time_between_purchases)] <- 40

# Assign 40 to user-item pairs with no purchases (thus no row in average_weeks_between_purchases and resulted in NA after the join)
summary(training_matrix$avg_time_between_purchases)
```

Has the user bought this item more than once?
```{r}
# Count purchases per User-Item Combo
purchase_counts <- orders %>%
  group_by(userID, itemID) %>%
  summarise(purchase_count = n()) %>%
  ungroup()

# binary indicator for multiple purchases
purchase_counts <- purchase_counts %>%
  mutate(multiple_purchases = if_else(purchase_count > 1, 1, 0))

# join back
training_matrix <- left_join(training_matrix, purchase_counts %>% dplyr::select(userID, itemID, multiple_purchases), by = c("userID", "itemID"))

# handle NAs
training_matrix$multiple_purchases[is.na(training_matrix$multiple_purchases)] <- 0

summary(training_matrix$multiple_purchases)

```

Has the user bought this item in December?
```{r}
# Filter for purchases made in December
december_purchases <- orders %>%
  filter(month(date) == 12) %>%
  distinct(userID, itemID) # Get unique user-item pairs for December

# Add a binary indicator for December purchases
december_purchases <- december_purchases %>%
  mutate(purchased_in_december = 1)

training_matrix <- left_join(training_matrix, december_purchases %>% dplyr::select(userID, itemID, purchased_in_december), by = c("userID", "itemID"))

training_matrix$purchased_in_december[is.na(training_matrix$purchased_in_december)] <- 0

summary(training_matrix$purchased_in_december)
```


### LASSO Training

Only want to train on some features
```{r}
# Example list of features you want to train on
selected_features <- c("userID", "itemID", "target", "total_count", "unique_purchase_weeks", "weeks_since_last_purchase", "avg_time_between_purchases", "multiple_purchases", "purchased_in_december")

# Create a new dataframe with only the selected features
training_data_selected <- training_matrix[selected_features]

```


Optimize feature selection with LASSO
```{r}
# Create matrix of predictor variables (exclude the target variable)
predictors <- dplyr::select(training_data_selected, -target) %>% as.matrix()

# Create a vector for the target variable
response <- training_data_selected$target
response <- factor(response, levels = c(0, 1, 2, 3, 4))  # Adjust levels accordingly
```

Cross validation for optimal lamda
```{r}
set.seed(10903866) 
cv_lasso <- cv.glmnet(predictors, response, alpha = 1, family = "multinomial")
```

Visualize optimal lamda and pick minimum lamda
```{r}
plot(cv_lasso)
best_lambda <- cv_lasso$lambda.min  # or lambda.1se for a more regularized model
```

Optimal lamda
```{r}
lasso_optimal <- glmnet(predictors, response, alpha = 1, family = "multinomial", lambda = best_lambda)
```

What are the coefficients
```{r}
coef(lasso_optimal)
```

### Predict Using LASSO and New Matrix

Preparing the test dataset
```{r}
# test needs to be joined with the matrix that is formed on January data as well...
```

Predict!
```{r}
predictors_names <- colnames(predictors)  # or manually list them if you've kept track

# Select the predictors from 'test_joined'
test_predictors <- as.matrix(test_joined[predictors_names])

# If you want the predicted class directly
predictions_class <- predict(lasso_optimal, newx = test_predictors, type = "class")

test_joined$target <- as.numeric(predictions_class)  # Convert to numeric and adjust range if needed

# Summary statistics for the target column
summary(test_joined$target)
```
What do the predictions look like?
```{r}
ggplot(test_joined, aes(x = factor(target))) +
  geom_bar(fill = "steelblue") +
  labs(title = "Distribution of Predicted Classes", x = "Class", y = "Count")
```

Converting the resulting predictions to submission csv
```{r}
# Assuming 'ID' is the name of your identifier column in `test_joined`
submission <- test_joined %>% select(uid_item_comb, target)

submission <- test_joined %>%
  # Replace underscores with dashes in uid_item_comb
  mutate(uid_item_comb = gsub("_", "-", uid_item_comb)) %>%
  # Select and rename columns
  select(ID = uid_item_comb, target)

write.csv(submission, "submission.csv", row.names = FALSE)
```


### Asking some questions for data integrity
How many purchases in test exist during January?
```{r}
# Filter for January purchases
january_orders <- orders %>%
  filter(month(date) == 1)

january_combinations <- january_orders %>%
  distinct(userID, itemID)

# Add a flag to indicate presence in January orders
test_with_january_flag <- test %>%
  mutate(in_january = ifelse(paste(userID, itemID) %in% paste(january_combinations$userID, january_combinations$itemID), 1, 0))

percentage_in_january <- mean(test_with_january_flag$in_january) * 100

print(paste("Percentage of test combinations present in December orders:", percentage_in_january, "%"))

```

```{r}
percentages <- c()
```



